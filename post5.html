<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
    <meta name="description" content="" />
    <meta name="author" content="" />
    <title>Fall 2022 Purdue CNIT 581-AST Project</title>
    <link rel="icon" type="image/x-icon" href="assets/favicon.ico" />
    <!-- Font Awesome icons (free version)-->
    <script src="https://use.fontawesome.com/releases/v6.1.0/js/all.js" crossorigin="anonymous"></script>
    <!-- Google fonts-->
    <link href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet"
        type="text/css" />
    <link
        href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800"
        rel="stylesheet" type="text/css" />
    <!-- Core theme CSS (includes Bootstrap)-->
    <link href="css/styles.css" rel="stylesheet" />
</head>

<body>
    <!-- Navigation-->
    <nav class="navbar navbar-expand-lg navbar-light" id="mainNav">
        <div class="container px-4 px-lg-5">
            <a class="navbar-brand" href="index.html">Purdue CNIT 581-AST / Fall 2022</a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarResponsive"
                aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                Menu
                <i class="fas fa-bars"></i>
            </button>
            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ms-auto py-4 py-lg-0">
                    <li class="nav-item"><a class="nav-link px-lg-3 py-3 py-lg-4" href="index.html">Home</a></li>
                    <li class="nav-item"><a class="nav-link px-lg-3 py-3 py-lg-4" href="about.html">About</a></li>
                </ul>
            </div>
        </div>
    </nav>
    <!-- Page Header-->
    <header class="masthead" style="background-image: url('assets/img/home-bg.jpg')">
        <div class="container position-relative px-4 px-lg-5">
            <div class="row gx-4 gx-lg-5 justify-content-center">
                <div class="col-md-10 col-lg-8 col-xl-7">
                    <div class="post-heading">
                        <h1>Week 15: Blog Post #5</h1>
                        <h2 class="subheading">Further Preprocessing & Zero-Shot Results</h2>
                        <span class="meta">
                            Posted by <a href="#">Nadine</a> on December 2, 2022
                        </span>
                    </div>
                </div>
            </div>
        </div>
    </header>
<!-- Post Content-->
    <article class="mb-4">
        <div class="container px-4 px-lg-5">
            <div class="row gx-4 gx-lg-5 justify-content-center">
                <div class="col-md-10 col-lg-8 col-xl-7">
                    <p>In this blog update, we will talk about the last data preprocessing step that we did on our dataset instances, the zero-shot results of the 
                      vision-and-language-transformer (ViLT) on our dataset, and what we are working on next.
                    </p>
                    <h4><strong>Further Data Preprocessing</strong></h4> 
                    <p>In <a href="https://nadine-amin.github.io/CNIT581-AST-blog/post3.html">blog post #3</a>, we demonstrated how we preprocessed our training and testing data 
                      by cropping each image such that the output is a 90x90 pixel image of just the face. After this step, each instance in our dataset is made up of a sequence of
                      several 90x90 pixel images of a certain participant while they are speaking out a certain utterance, one image per point in time. However, in order to input our 
                      data into the ViLT model, we would like to have one input image per instance. Consequently, we opted for concatenating, using 
                      <a href="https://note.nkmk.me/en/python-opencv-hconcat-vconcat-np-tile/">OpenCV</a>, all images resembling a certain utterance into one bigger image,
                      transforming the temporal information into spacial one <a href="http://cs231n.stanford.edu/reports/2016/pdfs/217_Report.pdf">[1]</a>.
                    </p>
                    <p>                      
                      Since different utterances have different lengths and different participants have different speaking speeds, not all instances are actually
                      made up of the same number of images. More specifically, the shortest instance is made up of just 4 images, while the longest one has 27 
                      images <a href="http://cs231n.stanford.edu/reports/2017/pdfs/227.pdf">[2]</a>. Following <a href="http://cs231n.stanford.edu/reports/2016/pdfs/217_Report.pdf">[1]</a>, 
                      our final concatenated image for each instance is made up of 5x5 smaller images (90x90 pixels each). Hence, the final dimensions of each 
                      concatenated image is 450x450 pixels. For instances which are shorter than 25 images, instead of appending black images (pixel values of 0) at the 
                      end of the sequence, we follow <a href="http://cs231n.stanford.edu/reports/2016/pdfs/217_Report.pdf">[1]</a> and stretch out the sequence of images 
                      filling the empty slots with the nearest image. More specifically, if the original sequence (orig_seq) has a length of orig_len, then the ith image 
                      in the stretched sequence (stretch_seq) is given by the following equation adapted from <a href="http://cs231n.stanford.edu/reports/2016/pdfs/217_Report.pdf">[1]</a>.
                    </p>
                    <center> <img class="img-responsive" src="assets/img/stretch_eq.png" alt="Image" width="500"> </center>
                    <p></p>
                  
                    <p> The figure below shows a sequence of images resembling a single data instance before and after this preprocessing step of stretching and 
                      concatenating. After this step, our data is now ready to be input into our model for fine-tuning and testing.
                    </p> 
                    <center> <img class="img-responsive" src="assets/img/stretch_concat_image.png" alt="Image" width="700"> </center>
                    <p></p>
                    
                    <h4><strong>Zero-Shot Results</strong></h4> 
                    <p> Before fine-tuning the ViLT model on our fine-tuning dataset and evaluating its performance on our testing dataset, we wanted to examine its 
                      zero-shot performance (before fine-tuning). Zero-shot evaluation is basically when a pre-trained model is tested on a dataset different than 
                        the one that has been used during its training. Consequently, we evaluated the classification performance of the pre-trained ViLT model 
                        on our testing dataset (1800 intances) in a multi-class classification problem with 20 classes (correspoding to the 20 different utterances in
                        our dataset). The classificaiton accuracy was found to be 4.83% and the below figure demonstrates the resulting confusion matrix.                  
                    </p> 
                    <img class="img-responsive" src="assets/img/zero_shot_conf_mat.png" alt="Image" width="700">
                    <p></p>
                
                    <p> As can be seen from the confusion matrix, either the label "Next" or "How are you?" was predicted for almost all of our testing instances. 
                        A couple of instances were instead predicted as "I am sorry." or "Good bye.". With a total of 20 classes, the zero-shot accuracy is actually 
                        equivalent to that of a random guesser (5%), which is definitely a really low accuracy.</p>
                    <p></p>
                    
                    <h4><strong>Next Steps</strong></h4> 
                    <p> We are currently working on fine-tuning the pre-trained ViLT model on our fine-tuning dataset so that we can then re-evaluate its performance
                        on our testing dataset. We will compare its performance with that of a random guesser and an encoder-decoder approach using CNN and LSTM.
                        We will also discuss possible future directions that we believe might be worth pursuing.
                    </p>
                                
                </div>
            </div>
        </div>
    </article>
    <!-- Footer-->
    <footer class="border-top">
        <div class="container px-4 px-lg-5">
            <div class="row gx-4 gx-lg-5 justify-content-center">
                <div class="col-md-10 col-lg-8 col-xl-7">
                    <ul class="list-inline text-center">
                        <li class="list-inline-item">
                            <a href="#!">
                                <span class="fa-stack fa-lg">
                                    <i class="fas fa-circle fa-stack-2x"></i>
                                    <i class="fab fa-twitter fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                        <li class="list-inline-item">
                            <a href="#!">
                                <span class="fa-stack fa-lg">
                                    <i class="fas fa-circle fa-stack-2x"></i>
                                    <i class="fab fa-facebook-f fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                        <li class="list-inline-item">
                            <a href="#!">
                                <span class="fa-stack fa-lg">
                                    <i class="fas fa-circle fa-stack-2x"></i>
                                    <i class="fab fa-github fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                    </ul>
                    <div class="small text-center text-muted fst-italic">Copyright &copy; 2022</div>
                </div>
            </div>
        </div>
    </footer>
    <!-- Bootstrap core JS-->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>
    <!-- Core theme JS-->
    <script src="js/scripts.js"></script>
</body>

</html>
