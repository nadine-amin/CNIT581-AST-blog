<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
    <meta name="description" content="" />
    <meta name="author" content="" />
    <title>Fall 2022 Purdue CNIT 581-AST Project</title>
    <link rel="icon" type="image/x-icon" href="assets/favicon.ico" />
    <!-- Font Awesome icons (free version)-->
    <script src="https://use.fontawesome.com/releases/v6.1.0/js/all.js" crossorigin="anonymous"></script>
    <!-- Google fonts-->
    <link href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet"
        type="text/css" />
    <link
        href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800"
        rel="stylesheet" type="text/css" />
    <!-- Core theme CSS (includes Bootstrap)-->
    <link href="css/styles.css" rel="stylesheet" />
</head>

<body>
    <!-- Navigation-->
    <nav class="navbar navbar-expand-lg navbar-light" id="mainNav">
        <div class="container px-4 px-lg-5">
            <a class="navbar-brand" href="index.html">Purdue CNIT 581-AST / Fall 2022</a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarResponsive"
                aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                Menu
                <i class="fas fa-bars"></i>
            </button>
            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ms-auto py-4 py-lg-0">
                    <li class="nav-item"><a class="nav-link px-lg-3 py-3 py-lg-4" href="index.html">Home</a></li>
                    <li class="nav-item"><a class="nav-link px-lg-3 py-3 py-lg-4" href="about.html">About</a></li>
                </ul>
            </div>
        </div>
    </nav>
    <!-- Page Header-->
    <header class="masthead" style="background-image: url('assets/img/home-bg.jpg')">
        <div class="container position-relative px-4 px-lg-5">
            <div class="row gx-4 gx-lg-5 justify-content-center">
                <div class="col-md-10 col-lg-8 col-xl-7">
                    <div class="post-heading">
                        <h1>Week 15: Blog Post #6</h1>
                        <h2 class="subheading">Fine-tuning Results & Final Demo Video!</h2>
                        <span class="meta">
                            Posted by <a href="#">Nadine</a> on December 9, 2022
                        </span>
                    </div>
                </div>
            </div>
        </div>
    </header>
<!-- Post Content-->
    <article class="mb-4">
        <div class="container px-4 px-lg-5">
            <div class="row gx-4 gx-lg-5 justify-content-center">
                <div class="col-md-10 col-lg-8 col-xl-7">
                    <p>In this last blog update, we will present and discuss the fine-tuning results of the vision-and-language transformer (ViLT) on our testing dataset, 
                    compare it with our baselines, show our demo video, and finally conclude our project and highlight potential future directions.
                    </p>
                    <h4><strong>Fine-tuning Results</strong></h4> 
                    <p> After assessing the zero-shot performance of the ViLT model in the <a href="https://nadine-amin.github.io/CNIT581-AST-blog/post5.html">previous blog post</a>,
                        we moved on to fine-tune our ViLT model on our fine-tuning dataset (6200 instances) as described in <a href="https://nadine-amin.github.io/CNIT581-AST-blog/post4.html">blog post 4</a>.
                        The figure below shows the loss curve during the fine-tuning process. As can be seen, the model was able to converge after around 1300 steps, which means
                        that the remaining steps might have been unnecessary.
                    </p>
                    <img class="img-responsive" src="assets/img/fine_tuning_loss_curve.png" alt="Image" width="700">
                    <p></p>
                    
                    <p> As with the zero-shot experiment, we test the fine-tuned ViLT model on our testing dataset (1800 intances) in a multi-class classification problem
                        with 20 classes (correspoding to the 20 different utterances in our dataset). The classificaiton accuracy was found to be 89.28%, which is a huge
                        improvement over the zero-shot results. The below figure demonstrates the resulting confusion matrix.
                    </p>
                    <img class="img-responsive" src="assets/img/fine_tuned_conf_mat.png" alt="Image" width="700">
                    <p></p>
                    
                    <p> We then compared our different testing results with those of our baselines: random guess and an encoder-decoder approach of a CNN and an LSTM by 
                        <a href="http://cs231n.stanford.edu/reports/2016/pdfs/217_Report.pdf">Garg et al. (2016)</a>. The figure below shows a summary of that comparison.
                        It seems like the fine-tuned ViLT model achieved a much higher accuracy compared to the baselines. However, there is a possibility that the model 
                        might have overfitted.
                    </p>
                    <img class="img-responsive" src="assets/img/results_comparison.png" alt="Image" width="700">
                    <p></p>
                  
                    <p> Check out our final demo video below where we perform several inferences using our fine-tuned ViLT model! From the video, we can see that the 
                        average inference time is as small as about 141 ms!
                    </p>
                    <iframe width="700" height="394" src="https://www.youtube.com/embed/MNaj7UjbewI" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>                    
                    <p></p>
                    
                    <h4><strong>Conclusions & Future Work</strong></h4> 
                    <p> As we have seen, our fine-tuned ViLT model was able to achieve an overall accuracy of almost 90% in the lip-reading task with an inference time
                        of about 150 ms. Although there is a possibility that the model has overfitted, this still demonstrates a great potential and hints that 
                        multi-modal models should be capable of performing lip-reading. It is worth noting that a more simplified procedure for the data preprocessing 
                        would be worth experimenting. In addition, this dataset might have been a little bit unbalanced. For instance, it has the following 3 utterances:
                        "Stop", "Navigation", and "Stop navigation", which means that the utterance "Stop navigation" might have had a few more fine-tuning instances.
                        
                        From the above conclusions, we put forward the following potential future directions:
                        <ul>
                            <li>We need to check whether the ViLT model has overfitted.</li>
                            <li>We can possibly try taking as input the video and directly performing 3D volume embedding in order to simplify the data preprocessing step.</li>
                            <li>We should probably use a better dataset that not only contains more instances, more words and more phrases, but that is also more balanced.</li>
                            <li>With this potential from the ViLT, other lightweight models should also be investigated.</li>
                            <li>We can move on to deploy the model onto portable devices.</li>
                        </ul>
                    
                    <p><a href="assets/final_presentation.pdf">Click Here</a> to see our full final presentation slides!</p>
                    </p>
                                
                </div>
            </div>
        </div>
    </article>
    <!-- Footer-->
    <footer class="border-top">
        <div class="container px-4 px-lg-5">
            <div class="row gx-4 gx-lg-5 justify-content-center">
                <div class="col-md-10 col-lg-8 col-xl-7">
                    <ul class="list-inline text-center">
                        <li class="list-inline-item">
                            <a href="#!">
                                <span class="fa-stack fa-lg">
                                    <i class="fas fa-circle fa-stack-2x"></i>
                                    <i class="fab fa-twitter fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                        <li class="list-inline-item">
                            <a href="#!">
                                <span class="fa-stack fa-lg">
                                    <i class="fas fa-circle fa-stack-2x"></i>
                                    <i class="fab fa-facebook-f fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                        <li class="list-inline-item">
                            <a href="#!">
                                <span class="fa-stack fa-lg">
                                    <i class="fas fa-circle fa-stack-2x"></i>
                                    <i class="fab fa-github fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                    </ul>
                    <div class="small text-center text-muted fst-italic">Copyright &copy; 2022</div>
                </div>
            </div>
        </div>
    </footer>
    <!-- Bootstrap core JS-->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>
    <!-- Core theme JS-->
    <script src="js/scripts.js"></script>
</body>

</html>
